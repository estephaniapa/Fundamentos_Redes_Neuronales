{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redes neuronales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<summary>\n",
    "    <font size=\"3\" color=\"darkorange\"> Proyecto integrador individual </font>\n",
    "</summary>\n",
    "\n",
    "---\n",
    "\n",
    "<summary>\n",
    "    <font size=\"4\">Propedeuticos: Maestría en Ciencias de Datos </font>\n",
    "</summary>\n",
    "<summary>\n",
    "    <font size=\"4\"> Universidad de Sonora </font>\n",
    "</summary>\n",
    "\n",
    "<summary>\n",
    "    <font size=\"4\" color=\"deeppink\"> Estephania Pivac Alcaraz </font>\n",
    "</summary>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Definción y ejemplos de redes neuronales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Introducción a las redes neuronales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Inteligencia Artificial (AI)**\n",
    "\n",
    "Definir la Inteligencia Artificial es tan difícil como definir la inteligencia humana. Sin embargo, en esta introducción, me daré a la tarea de mencionar sus características principales para dar una noción de lo que es la AI.\n",
    "\n",
    "La inteligencia artificial (AI por sus siglas en inglés), involucra el uso de computadoras para realizar tareas que tradicionalmente requieren inteligencia humana. Estas tareas son tan variadas que pueden involucrar la capacidad de cálculo, de memorizar datos, de aprender, la creatividad o la autoconsciencia.\n",
    "\n",
    "Podemos pensae que la AI se refiere al diseño de programas que tomen automáticamente decisiones con respecto a alguna tarea, sin ser programados explícitamente para tomar decisiones concretas.\n",
    "\n",
    "#### **Aprendizaje Automático (ML)**\n",
    "\n",
    "Dentro de la AI podemos encontrar el área de Aprendizaje Automático (ML por sus siglas en inglés: Machine Learning), en el cual, el objetivo es que los modelos aprendan por sí solos en base a un gran conjunto de datos. Para ello se utilizan técnicas estadísticas para encontrar patrones en los datos y poder hacer predicciones sobre nuevos datos sin programarlos explícitamente.\n",
    "\n",
    "El Aprendizaje Automático (Machine Learning) se encarga del estudio y uso de algoritmos para analizar datos, aprender de ellos y a partir de esto obtener conclusiones o predicciones para nuevos datos. \n",
    "\n",
    "La diferencia fundamental entre la programación tradicional y Machine Learning, tiene que ver con el **aprendendizaje a partir de los datos**. Con ML en lugar de dar explícitamente un conjunto de instrucciones específicas para completar una tarea particular, la máquina se entrena utilizando un conjunto grande de datos y algoritmos que permiten a la máquina la habilidad de aprender cómo hacer dicha tarea sin decirle explícitamente cómo hacerlo.\n",
    "\n",
    "**Tipos de aprendizaje**\n",
    "\n",
    "Nuestro cerebro nos permite abordar casi cualquier situación y aprender a lidiar con ella sin tener instrucciones explícitas, y en base a la experiencia adquirida vamos mejorando nuestras habilidades.\n",
    "\n",
    "Esta idea de aprendizaje se reproduce en ML mediante dos tipos de aprendizaje: aprendizaje supervisado y aprendizaje no supervisado.\n",
    "\n",
    "\n",
    "**Aprendizaje supervisado**\n",
    "\n",
    "El *aprendizaje supervisado* se basa en realizar predicciones hechas a partir de un modelo construido con datos etiquetados con los valores a predecir. Esto es, conocemos de antemano a qué categoría o predicción corresponde cada elemento del conjunto de datos. Se llama supervisado porque en el proceso de aprendizaje el modelo utiliza la información de entrada para ajustar sus pesos. \n",
    "\n",
    "Debemos evitar dos situaciones. La primera situación es cuando el modelo se aprende exactamente todos los valores de entrada, y funciona perfectamente con estos valores, pero no es capaz de realizar predicciones de nuevos datos, a esto se le conoce como **overfitting**. El caso contrario, llamado **underfitting** es cuando no completamos el entrenamiento con los datos de entrada y el modelo no tiene la capacidad para realizar tales predicciones. Estas situaciones afectan o limitan la capacidad del modelo para generalizar.\n",
    "\n",
    "**Aprendizaje no supervisado**\n",
    "\n",
    "El *aprendizaje no supervisado* por su parte se encarga de analizar y agrupar datos sin etiquetar. Este tipo de aprendizaje busca detectar características o patrones y detectar anomalías. \n",
    "\n",
    "#### **Deep Learning (DL)**\n",
    "Deep Learning es un campo dentro de Machine Learning que utiliza algoritmos inspirados por la estructura y funcionamiento de las redes neuronales del cerebro. \n",
    "\n",
    "En DL, las redes neuronales artificiales, de las cuales hablaremos más adelante, aprenden de grandes cantidades de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Redes neuronales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las redes neuronales, también conocidas como redes neuronales artificiales (ANN) o redes neuronales simuladas (SNN), son sistemas computacionales que imitan las capacidades cerebrales. Se trata de un conjunto de nodos simples o neuronas conectadas entre sí que trabajan conjuntamente, y que van creando conexiones y reforzando otras consiguiendo gran capacidad de procesamiento.\n",
    "\n",
    " Las redes neuronales pueden adaptarse a cambios de entrada, de manera que la red genera el mejor resultado posible sin necesidad de rediseñar los criterios de salida.\n",
    "\n",
    "Las redes neuronales artificiales, se componen de capas de nodos, que contienen una capa de entrada, una o más capas ocultas, y una capa de salida. Cada nodo, o neurona artificial, se conecta a otro y tiene un peso y un umbral asociados. Si la salida de cualquier nodo individual está por encima del valor de umbral especificado, dicho nodo se activa, enviando datos a la siguiente capa de la red. De lo contrario, no se pasan datos a la siguiente capa de la red."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1 Perceptrón simple o red neuronal monocapa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El perceptrón es la red neuronal más básica, también conocida como neuron artificial, consta de un solo nodo con varias entradas y una única salida. En este nodo se se multiplican todas las entradas por sus pesos correspondientes y se suman, por lo que está limitado a problemas de clasificación lineal, es decir, encontrar un hiperplano que separe los patrones en dos clases. Esta neurona proporciona una salida distinte segun la función de activación.\n",
    "\n",
    "<center><img src=\"Imagen1.png\" width=600></center>\n",
    "\n",
    "Podemos pensar al perceptrón o la neurona como simples procesadores de información, que tiene múltiples entradas $x_i$, $i=1, 2, \\ldots n$ y una salida $y$. \n",
    "\n",
    "**Componentes de un perceptrón**\n",
    "\n",
    "<center><img src=\"Imagen2.png\" width=600> </center>\n",
    "\n",
    "1. **Entradas $x_i$:**  Una neurona artificial recibe una serie de entradas numéricas. Corresponden a $x_1, x_2, \\ldots, x_n$.\n",
    "\n",
    "2. **Pesos $w_i$:** Cada entrada de la neurona tiene asociado un peso, que representa la fuerza de la conexión entre la entrada y la neurona. Estos pesos ayudarán a la red neuronal a aprender. Los pesos determinan la contribución relativa de cada entrada en la salida de la neurona. Se pueden inicializar de forma aleatoria y se irán refinando durante el entrenamiento.\n",
    "\n",
    "3. **Bias o sesgo $b$:**  Además de las entradas y los pesos, una neurona puede tener un sesgo (bias) adicional. El sesgo es un parámetro que se suma a la suma ponderada antes de aplicar la función de activación. El sesgo permite ajustar el punto de activación de la neurona y controlar el umbral de activación.\n",
    "\n",
    "4. **Procesamiento lineal**\n",
    "Las entradas y los pesos se multiplican y suman, para calcular la suma ponderada de las entradas. Esta suma ponderada es una combinación lineal de las entradas y los pesos, y se le agrega el bias.\n",
    "$$z = \\sum_{i=1}^n x_i w_i + b$$\n",
    "\n",
    "\n",
    "\n",
    "4. **Función de activación $f$:**  \n",
    "La función de activación es una función matemática que activa los pesos de una neurona en base al resultado de la computación de esa neurona.\n",
    "La suma ponderada obtenida del procesamiento lineal se pasa a través de una función de activación, que introduce no linealidad en la salida de la neurona. Esta determina el rango y la forma de la salida de la neurona.  Las funciones de activación son esenciales en las redes neuronales para introducir no linealidad y permitir el aprendizaje de relaciones complejas en los datos. Permiten una mayor representación y expresividad, y son fundamentales para el éxito y la eficacia de las redes neuronales en una amplia gama de aplicaciones.\n",
    "$$f(z) = f(\\sum_{i=1}^n x_i w_i + b)$$\n",
    "\n",
    "5. **Salida $y$:** La salida de la neurona artificial, la variable dependiente $y$, se calcula aplicando la función de activación a la suma ponderada más el sesgo. La salida puede ser la salida final de la neurona o puede ser la entrada para otras neuronas en una red neuronal más grande.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2 Perceptrón multi-layer o red neuronal multicapa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Son un tipo de red neuronal parecido al anterior pero con al menos una capa de neuronas entre la capa de entrada y la capa de salida. A estas capas intermedias se les denomina ocultas y la profundidad está determinado por el número de capas ocultas que tenga la red. Mientras más capas ocultas tenga una red neuronal, más profundidad tendrá y mayor capacidad de generalización, pero puede desencadenar un problema de overfitting como ya hemos mencionado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.3 Componentes de una red neuronal\n",
    "\n",
    "Una red neuronal consiste en un largo número de unidades simples llamadas neuronas, que reciben y transmiten señales unas con otras. Es decir, consisten de capas de neuronas, y las redes neuronales artificiales multi-capas pueden resolver problemas no linealmente separables.\n",
    "\n",
    "A menudo las arquitecturas de redes neuronales están compuestas por capas de neuronas que se comunican entre sí, principalmente las podemos dividir en:\n",
    "\n",
    "1. **Capa de entrada:** Es la capa inicial de la red neuronal donde se introducen los datos de entrada. Cada neurona en esta capa representa una característica o variable de entrada. No hay cálculos o transformaciones en esta capa, solo se transmiten las entradas a la siguiente capa.\n",
    "\n",
    "2. **Capas ocultas:** Son capas intermedias entre la capa de entrada y la capa de salida. Estas capas realizan cálculos y transformaciones complejas para procesar la información de entrada. Pueden haber una o varias capas ocultas en una red neuronal, y el número de neuronas en cada capa oculta puede variar.\n",
    "\n",
    "3. **Capa de salida:** Es la capa final de la red neuronal donde se obtienen las salidas predichas. La forma y el número de neuronas en esta capa dependen del problema que se esté abordando. Por ejemplo, en problemas de clasificación binaria, se puede utilizar una neurona con una función de activación sigmoide para producir una salida entre 0 y 1. En problemas de clasificación multiclase, se puede utilizar una neurona por clase con una función de activación softmax.\n",
    "\n",
    "<center><img src=\"Imagen6.png\" width=300> </center>\n",
    "\n",
    "La capa de entrada es donde el modelo ingiere los datos para posteriormente transformarlos en las capas ocultas y la capa final es donde se realiza la predicción o clasificación final."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.4 Ejemplos de arquitecturas de redes neuronales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"image7.png\" width=600> </center>\n",
    "\n",
    "Existen varias arquitecturas de redes neuronales, cada una diseñada para abordar diferentes tipos de problemas y aprender patrones específicos en los datos. A continuación, se presentan algunas de las arquitecturas más comunes de redes neuronales:\n",
    "\n",
    "##### Redes Neuronales Feedforward (FFNN)\n",
    "\n",
    "Una red neuronal feedforward, también conocida como red neuronal de propagación hacia adelante, es un tipo de red neuronal artificial que se utiliza en la mayoría de las aplicaciones de aprendizaje automático y reconocimiento de patrones. Es una red neuronal en la que la información fluye en una sola dirección, desde la entrada hasta la salida, sin bucles ni conexiones hacia atrás.\n",
    "\n",
    "La red neuronal feedforward consta de múltiples capas:\n",
    "- una capa de entrada\n",
    "- una o más capas ocultas\n",
    "- una capa de salida\n",
    "\n",
    "Cada capa consta de un conjunto de nodos o neuronas, que reciben entradas de la capa anterior y generan salidas para la capa siguiente. Los pesos entre los nodos se ajustan durante el entrenamiento de la red neuronal para minimizar el error en la predicción de la salida.\n",
    "\n",
    "El proceso de predicción en una red neuronal feedforward implica alimentar los datos de entrada a través de la capa de entrada y las capas ocultas, y finalmente producir una salida en la capa de salida. Cada capa de la red neuronal utiliza una función de activación no lineal para transformar las entradas y generar las salidas correspondientes. Las funciones de activación más comunes son la función sigmoidal y la función ReLU.\n",
    "\n",
    "Las redes neuronales feedforward son utilizadas en una amplia variedad de aplicaciones de aprendizaje automático, incluyendo la clasificación de imágenes, el procesamiento del lenguaje natural, el reconocimiento de voz, la detección de fraudes, la predicción de series temporales, entre otros.\n",
    "\n",
    "\n",
    "##### Redes Neuronales Recurrentes (RNN)\n",
    "\n",
    "<center><img src=\"image8.jpeg\" width=600> </center>\n",
    "\n",
    "Una red neuronal recurrente (RNN) es un tipo de red neuronal artificial que utiliza datos secuenciales o datos de series de tiempo. Estos algoritmos de aprendizaje profundo se utilizan comúnmente para problemas ordinales o temporales, como la traducción de idiomas, el reconocimiento de voz y subtítulos de imágenes.\n",
    "\n",
    "Cabe mencionar que están incorporados en aplicaciones populares como Siri, búsqueda por voz y Google Translate.\n",
    "Al igual que las redes neuronales feedforward y convolucionales (CNN), las redes neuronales recurrentes utilizan datos de entrenamiento para aprender. Se distinguen por su \"memoria\", ya que toman información de entradas anteriores para utilizarse en los datos de entrada y en los resultados.\n",
    "\n",
    "Si bien las redes neuronales profundas tradicionales asumen que los datos de entrada y los resultados son independientes entre sí, los resultados de las redes neuronales recurrentes depende de los elementos anteriores dentro de la secuencia. Aunque los eventos futuros también serían útiles para determinar los resultados de una secuencia dada, las redes neuronales recurrentes unidireccionales no pueden tener en cuenta estos eventos en sus predicciones.\n",
    "\n",
    "\n",
    "##### Redes Neuronales Convolucionales (CNN)\n",
    "\n",
    "Las redes neuronales convolucionales pueden tener decenas o cientos de capas, y cada una de ellas aprende a detectar diferentes características de una imagen. Se aplican filtros a las imágenes de entrenamiento con distintas resoluciones, y la salida resultante de convolucionar cada imagen se emplea como entrada para la siguiente capa. Los filtros pueden comenzar como características muy simples, tales como brillo y bordes, e ir creciendo en complejidad hasta convertirse en características que definen el objeto de forma singular.\n",
    "\n",
    "Una CNN consta de una capa de entrada, una capa de salida y varias capas ocultas entre ambas.\n",
    "Estas capas realizan operaciones que modifican los datos, con el propósito de comprender sus características particulares.\n",
    "\n",
    "<center><img src=\"image12.png\" width=600> </center>\n",
    "\n",
    "- Capa de entrada: La capa de entrada recibe la imagen de entrada y puede aplicar algunas transformaciones iniciales, como redimensionar la imagen o normalizar los valores de píxeles.\n",
    "\n",
    "- Capas convolucionales: Las capas convolucionales son el componente principal de una CNN y se utilizan para extraer características de las imágenes. Cada capa convolucional aplica múltiples filtros (kernels) que se deslizan por la imagen y realizan operaciones de convolución para detectar características locales, como bordes, texturas o formas. Estas capas convolucionales generan mapas de características convolucionales, que capturan información relevante de la imagen a diferentes niveles de abstracción.\n",
    "\n",
    "- Capas de activación: Después de cada capa convolucional, se aplica una función de activación no lineal, como la función ReLU (Rectified Linear Unit), para introducir no linearidades en la red y permitir la representación de relaciones no lineales en los datos.\n",
    "\n",
    "- Capas de agrupación (Pooling): Las capas de agrupación se utilizan para reducir la dimensionalidad de los mapas de características y hacer que la red sea más robusta ante pequeñas variaciones en la posición de las características detectadas.\n",
    "La operación de agrupación toma una región de los mapas de características y aplica una función de agregación, como el máximo o el promedio, para obtener un único valor representativo de esa región.\n",
    "\n",
    "- Capas totalmente conectadas: Después de las capas convolucionales y de agrupación, se pueden agregar una o varias capas totalmente conectadas, similares a las de una red neuronal tradicional. Estas capas toman los mapas de características resultantes y los \"aplanan\" en un vector unidimensional, que luego se alimenta a través de capas densamente conectadas. Estas capas completamente conectadas aprenden a combinar y clasificar las características extraídas por las capas anteriores para producir las salidas deseadas, como etiquetas de clase en un problema de clasificación.\n",
    "\n",
    "- Capa de salida: La capa de salida generalmente consiste en una o varias neuronas que representan las clases o valores de salida deseados. En problemas de clasificación, se utiliza una función de activación softmax para obtener una distribución de probabilidades sobre las clases. En problemas de regresión, se puede utilizar una función de activación lineal o una función no lineal adecuada, según el rango de valores de salida requerido.\n",
    "\n",
    "\n",
    "Cabe mencionar que la estructura exacta de una red neuronal convolucional puede variar dependiendo de la arquitectura específica utilizada, como LeNet, AlexNet, VGGNet, ResNet, entre otras. Cada arquitectura puede tener una cantidad diferente de capas, así como también puede incluir capas adicionales o técnicas avanzadas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Esquema matemático que involucran las redes neuronales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Definición formal de red neuronal\n",
    "\n",
    "Para obtener una definición de red neuronal tenemos que hacer uso del concepto matemático de grafo. A través de este término, podemos definir una red neuronal de la siguiente forma:\n",
    "\n",
    "**Definición**\n",
    "Una red neuronal es un grafo dirigido con las siguientes propiedades:\n",
    "\n",
    "1. A cada nodo i se le asocia un variable de estado $x_i$.\n",
    "2. A cada conexión (i, j) de los nodos i y j se el asocia un peso $w_{ij} \\in \\mathbb{R}$.\n",
    "3. A cada nodo i se le asocia un umbral $\\theta_i$.\n",
    "4. Para cada nodo i se define una función $f_i(x_i, w_{ij}, \\theta_i)$, que depende de los pesos de sus conexiones, del umbral y de los estados de los nodos j a él conectados. Esta función proporciona el nuevo estado del nodo.\n",
    "\n",
    "\n",
    "Cuando trabajamos con redes neuronales el esquema matemático a seguir es el siguiente. Comenzamos explorando lo que sucede con una sola neurona."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Suma ponderada\n",
    "Las señales de entrada $\\vec{x}=(x_1, x_2, \\ldots, x_n)$, la información que recibe nuestra neuronal artificial, son variables independientes. Los n-valores de entrada son multiplicados por sus respectivos pesos, es decir en la sinopsis el vector entrada es multiplicado por el vector peso, dando como resultado una combinación lineal de las entradas y los pesos, algo que denominamos suma ponderada.\n",
    "$$\\vec{x} \\cdot \\bf{\\vec{w}^T} = (x_1, x_2, \\ldots x_n) \\cdot \\begin{pmatrix}\n",
    "    w_1 \\\\\n",
    "    w_2 \\\\\n",
    "    \\vdots \\\\\n",
    "    w_n \\\\\n",
    "\\end{pmatrix}\n",
    "= \\sum_{i=1}^n x_i w_i$$\n",
    "\n",
    "Además de las entradas y los pesos, una neurona puede tener un sesgo (bias) adicional. En el esquema matemático de una neurona, el bias se agrega como un término adicional en la suma ponderada de las entradas antes de aplicar la función de activación. El valor del bias se establece de manera aleatoria en un primer momento y se ajusta durante el proceso de entrenamiento de la red neuronal junto con los pesos sinápticos. Al ajustar el valor del bias, se puede lograr un mejor ajuste y rendimiento de la red neuronal en la tarea específica que está realizando. Enseguida agregamos el factor bias o sesgo $w_0$.\n",
    "$$w_0 + \\vec{x} \\cdot \\bf{\\vec{w}^T}\n",
    "= w_0 + \\sum_{i=1}^n x_i w_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Funciones de activación\n",
    "La suma ponderada se pasa a través de una función de activación, que introduce no linealidad en la salida de la neurona. Esta determina el rango y la forma de la salida de la neurona. La función de activación permite crear estructuras más complejas de neuronas secuenciales conectadas sin que \"colapsen\", ya que sin la función de activación, una red neuronal se reduciría a una combinación lineal de las entradas y los pesos, lo que limitaría su capacidad de aprendizaje y representación de datos complejos.\n",
    "Aplicamos la función de activación:\n",
    "$$f(w_0 + \\vec{x} \\cdot \\bf{\\vec{w}^T})$$\n",
    "\n",
    "Y por último dicho resultado se propaga a la salida. Dicho valor puede ser la nueva entrada de una neurona, formando así las redes neuronales, o bien puede ser el resultado final, nuestra variable respuesta.\n",
    "\n",
    "    \n",
    "Existen distintas funciones de activación, a continuación presente algunos ejemplos con casos de uso comunes:\n",
    "    <center><img src=\"Imagen3.png\" width=600> </center>\n",
    "    \n",
    "    \n",
    "- Función Sigmoide:  La función sigmoide se utiliza principalmente en la capa de salida para clasificación binaria. $$f(x) = \\sigma (x) = \\frac{1}{1+e^{-x}}$$\n",
    "\n",
    "- Función tangente hiperbólica $\\tanh$: La función tangente hiperbólica es útil en capas ocultas de redes neuronales, especialmente en problemas de clasificación donde las salidas pueden tener tanto valores positivos como negativos.  $$f(x) = \\tanh(x) = \\frac{e^x - e^{-x}}{e^x+e^{-x}}$$\n",
    "\n",
    "- Función ReLU: La función ReLU es ampliamente utilizada en las capas ocultas de las redes neuronales, especialmente en arquitecturas más profundas. Es menos propensa al problema de desvanecimiento de gradientes y puede acelerar el proceso de entrenamiento.\n",
    "$$f(x) = \\max(0,x)$$\n",
    "\n",
    " - Función Softmax: La función de activación softmax se utiliza comúnmente en la capa de salida de una red neuronal cuando se está abordando un problema de clasificación multiclase. La función softmax produce una distribución de probabilidades entre las clases, asignando un valor entre 0 y 1 a cada clase, de modo que la suma de todas las salidas sea igual a 1.\n",
    "    $$f(x_i)=\\frac{e^{x_i}}{\\sum_{j=1}^{n}e^{x_j}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Función de pérdida\n",
    "Es importante establecer cómo vamos a medir el desempeño de la red neuronal, para esto necesitamos definir cómo mediremos los errores. La función de pérdida es una parte fundamental cuando utilizamos modelos de redes neuronales porque determina el objetivo del algoritmo.\n",
    "$$Loss(\\hat{y}_{i}, y_{i})$$\n",
    "\n",
    "La \"loss function\" (función de pérdida) en las redes neuronales es una función que cuantifica el error entre las salidas predichas por la red y las salidas objetivo o etiquetas conocidas. El objetivo del entrenamiento es minimizar la función de pérdida promedio, es decir, reducir la discrepancia entre las salidas predichas y las salidas objetivo.\n",
    "\n",
    "$$AverageLoss\\Big (\\frac{1}{n} \\sum_{i=1}^n Loss( \\hat{y}_{i}, y_{i})\\Big)$$\n",
    "\n",
    "Existen diferentes funciones de pérdida, algunos ejemplos comunes son las siguientes:\n",
    "\n",
    "- Error cuadrado medio (MSE): Pone gran énfasis la magnitud promedio del error, independientemente de su dirección. Sin embargo, debido al cuadrado, las predicciones que están lejos de los valores reales son penalizadas en gran medida en comparación con las predicciones menos desviadas. Además, el MSE tiene propiedades matemáticas favorables que facilitan el cálculo de los gradientes. $$MSE = \\frac{ \\sum_{i=1}^n ( y_{i} - \\hat{y}_{i})^2}{n}$$\n",
    "\n",
    "- Error absoluto medio (MAE): El error absoluto medio (MAE, por sus siglas en inglés), por otro lado, se mide como el promedio de la suma de las diferencias absolutas entre las predicciones y las observaciones reales. Al igual que el error cuadrático medio (MSE), esto también mide la magnitud del error sin tener en cuenta su dirección. A diferencia del MSE, el MAE requiere herramientas más complicadas, como la programación lineal, para calcular los gradientes. Además, el MAE es más robusto ante valores atípicos ya que no utiliza el cuadrado $$MAE = \\frac{ \\sum_{i=1}^n | y_{i} - \\hat{y}_{i}|}{n}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Entrenamiento de la red neuronal (algoritmos)\n",
    "El objetivo del algoritmo de entrenamiento de redes neuronales es ajustar los pesos sinápticos y los sesgos de la red de manera que la red pueda aprender y generalizar a partir de los datos de entrenamiento. El algoritmo busca encontrar una configuración de pesos y sesgos que minimice la diferencia entre las salidas predichas por la red y las salidas objetivo deseadas.\n",
    "\n",
    "El proceso de entrenamiento implica presentar iterativamente los ejemplos de entrenamiento a la red neuronal, propagar hacia adelante las entradas a través de las capas de la red para calcular las salidas predichas y luego comparar esas salidas con las salidas objetivo. A partir de la diferencia entre las salidas predichas y las salidas objetivo, se calcula una función de pérdida o error que cuantifica qué tan bien está realizando la red en la tarea.\n",
    "\n",
    "El algoritmo de entrenamiento utiliza el error calculado para ajustar los pesos y sesgos en sentido contrario al gradiente de la función de pérdida. Esto se conoce como descenso de gradiente y se realiza mediante el algoritmo de backpropagation. El objetivo es encontrar una configuración de pesos y sesgos que minimice el error de manera gradual a medida que se presentan más ejemplos de entrenamiento.\n",
    "\n",
    "En resumen, el objetivo del algoritmo de entrenamiento de redes neuronales es optimizar los parámetros de la red para que pueda realizar predicciones precisas y generalizar a partir de los datos de entrenamiento, es decir, ser capaz de realizar predicciones precisas en datos no vistos anteriormente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Forward propagation\n",
    "El proceso de propagación hacia adelante (forward propagation en inglés) en una red neuronal se basa en realizar una serie de cálculos matemáticos para propagar la información de entrada a través de las capas de la red y obtener una salida final.\n",
    "\n",
    "A continuación, se presenta un esquema matemático simplificado del proceso de propagación hacia adelante en una red neuronal de alimentación directa (feedforward) con una capa oculta:\n",
    "\n",
    "Asigna las entradas:\n",
    "\n",
    "$$X = (x_1, x_2, x_3, ..., x_n)$$ \n",
    "(vector de características de entrada)\n",
    "\n",
    "Calcula la salida de la primera capa oculta:\n",
    "$$Z^{[1]} = W^{[1]} \\cdot X + b^{[1]}$$\n",
    "\n",
    "$$A^{[1]} = \\sigma(Z^{[1]})$$\n",
    "donde:\n",
    "- $W^{[1]}$ es la matriz de pesos de la capa oculta\n",
    "- $b^{[1]}$ es el vector de sesgos de la capa oculta\n",
    "- $\\sigma$ es la función de activación de la capa oculta\n",
    "- $Z^{[1]}$ es el vector que representa la combinación lineal de las entradas en la capa oculta\n",
    "- $A^{[1]}$ es el vector de salida de la capa oculta\n",
    "\n",
    "\n",
    "Como estamos considerando solo una capa oculta, la salida de esta capa $A_1$ representa la entrada a la capa de salida. La idea dentro de forward propagation es hacer los cálculos matemáticos para obtener el valor de la variable de salida. Calculamos la salida de la capa de salida:\n",
    "\n",
    "$$Z^{[2]} = W^{[2]} * A^{[1]} + b^{[2]}$$\n",
    "$$A^{[2]} = \\sigma (Z^{[2]})$$\n",
    "donde:\n",
    "- $W^{[2]}$ es la matriz de pesos de la capa de salida\n",
    "- $b^{[2]}$ es el vector de sesgos de la capa de salida\n",
    "- $\\sigma$ es la función de activación de la capa de salida\n",
    "La salida final de la red neuronal es $A^{[2]}$, que representa las predicciones o salidas del modelo.\n",
    "\n",
    "El proceso de propagación hacia adelante en una red neuronal implica calcular la salida de cada capa a partir de las entradas y los pesos correspondientes, aplicar una función de activación a cada salida para introducir no linealidad, y repetir este proceso para cada capa hasta llegar a la capa de salida. La salida final de la red es el resultado de la última capa de activación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Backpropagation\n",
    "\n",
    "<center><img src=\"backpropagation.png\" width=600> </center>\n",
    "\n",
    "Backpropagation (retropropagación del error) es un algoritmo fundamental utilizado en el entrenamiento de redes neuronales para ajustar los pesos sinápticos de manera eficiente. Permite calcular el gradiente de la función de pérdida o error con respecto a los pesos de la red, lo que facilita la actualización de los pesos en la dirección que reduce el error de predicción utilizando algún método de optimización como el descenso de gradientes o alguna variante de éste.\n",
    "\n",
    "El algoritmo de retropropagación, abreviado como \"backward propagation of errors\" (propagación hacia atrás de errores), es un algoritmo para el aprendizaje supervisado de redes neuronales artificiales utilizando descenso de gradiente. Dada una red neuronal artificial y una función de error, el método calcula el gradiente de la función de error con respecto a los pesos de la red neuronal.\n",
    "\n",
    "El cálculo del gradiente se realiza mediante la regla de la cadena (chain rule) de cálculo diferencial. El gradiente se calcula en sentido inverso, comenzando desde la capa de salida y retrocediendo hacia la capa de entrada. Para cada neurona, se calcula el gradiente del error con respecto a los pesos sinápticos y se actualizan dichos pesos en dirección opuesta al gradiente.\n",
    "\n",
    "El objetivo del algoritmo de backpropagation es minimizar la función de pérdida ajustando los pesos de manera iterativa. Esto se logra actualizando los pesos utilizando un algoritmo de optimización, como el descenso del gradiente, que modifica los pesos en función de la magnitud del gradiente y una tasa de aprendizaje.\n",
    "\n",
    "La retropropagación del error es esencial para el entrenamiento de redes neuronales profundas, ya que permite el ajuste eficiente de los pesos en todas las capas de la red. Al propagar el error desde la capa de salida hasta la capa de entrada, la red puede aprender a realizar predicciones más precisas y ajustarse a los patrones de los datos de entrenamiento.\n",
    "\n",
    "Para utilizar backpropagation necesitamos tres elementos:\n",
    "\n",
    "1. Conjunto de datos que consiste de pares ordenados de entradas-salidas $(\\vec{x}_i, \\vec{y}_i)$, donde $\\vec{x}_i$ es la entrada y $\\vec{y}_i$ la salida deseada para dicha entrada $\\vec{x}_i$. Al conjunto de parejas de entrada-salida de tamaño $N$ se denota $X= {(\\vec{x}_1, \\vec{y}_1), \\ldots, (\\vec{x}_N, \\vec{y}_N) }$.\n",
    "\n",
    "2. Una red neuronal feedforward, cuyos parámetros en conjunto se denotan por $\\theta$. En backpropagation, los parámetros de principal interés son $w^k_{ij}$, los pesos entre el nodo $j$ en la capa $l_k$ y el nodo $i$ en la capa $l_{k-1}$, y $b_i^k$, el sesgo para el nodo $i$ en la capa $l_k$. No hay conexiones entre nodos en la misma capa y las capas están fully connected.\n",
    "\n",
    "3. Una función de error, $E(X,\\theta)$, la cual define el error entre el output deseado $\\vec{y}_i$ y el calculado por la red neuronal $\\hat{\\vec{y}}_i$ para la entrada $\\vec{x}_i$ para un conjunto de pares entrada-salida $(\\vec{x}_i, \\vec{y}_i)\\in X$ y un valor particular de los parámetros $\\theta$.\n",
    "\n",
    "Entrenar una red neuronal con descenso de gradiente requiere calcular los gradientes de la función de error $E(X,\\theta)$ con respecto a los pesos $w^k_{ij}$ y sesgos $b_i^k$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8 Descenso de gradientes\n",
    "\n",
    "Entrenar una red neuronal con descenso de gradiente requiere calcular los gradientes de la función de error $E(X,\\theta)$ con respecto a los pesos $w^k_{ij}$ y sesgos $b_i^k$. Entonces, de acuerdo a la tasa de aprendizaje $\\alpha$, cada iteración de descenso de gradiente ajusta los pesos y sesgos de acuerdo a:\n",
    "\n",
    "$$\\theta^{t+1} = \\theta^1 - \\alpha \\frac{\\partial E(X,\\theta^t)}{\\partial \\theta}$$\n",
    "\n",
    "donde $\\theta^t$ denota los parámetros de la red neuronal en la iteración t del descenso de gradiente.\n",
    "\n",
    "Antes de sumergirnos en el descenso de gradiente, puede ser útil revisar algunos conceptos de la regresión lineal. Puede recordar la siguiente fórmula para la pendiente de una línea, que es y = mx + b, donde m representa la pendiente y b  es la intersección en el eje y.\n",
    "\n",
    "También puede recordar trazar un diagrama de dispersión en estadísticas y encontrar la línea de mejor ajuste, lo que requirió calcular el error entre la salida real y la salida predicha (y-hat) usando la fórmula del error cuadrático medio. El algoritmo de descenso de gradiente se comporta de manera similar, pero se basa en una función convexa.\n",
    "\n",
    "El punto de partida es solo un punto arbitrario para que podamos evaluar el rendimiento. Desde ese punto de partida, encontraremos la derivada (o pendiente), y desde allí, podemos usar una recta tangente para observar la inclinación de la pendiente. La pendiente informará las actualizaciones de los parámetros, es decir, los pesos y el sesgo. La pendiente en el punto de partida será más pronunciada, pero a medida que se generen nuevos parámetros, la pendiente debe reducirse gradualmente hasta alcanzar el punto más bajo de la curva, conocido como punto de convergencia.   \n",
    "\n",
    "De manera similar a encontrar la línea de mejor ajuste en la regresión lineal, el objetivo del descenso de gradiente es minimizar la función de costo, o el error entre y pronosticada y real. Para hacer esto, se necesitan dos puntos de datos: una dirección y una tasa de aprendizaje. Estos factores determinan los cálculos de derivadas parciales de iteraciones futuras, lo que le permite llegar gradualmente al mínimo local o global (es decir, punto de convergencia).\n",
    "\n",
    "Tasa de aprendizaje (también conocida como tamaño de paso o alfa)  es el tamaño de los pasos que se dan para alcanzar el mínimo. Suele ser un valor pequeño y se evalúa y actualiza en función del comportamiento de la función de costos. Elevado  Las tasas de aprendizaje dan como resultado pasos más grandes, pero se corre el riesgo de sobrepasar el mínimo. Por el contrario, una tasa de aprendizaje baja  tiene tamaños de paso pequeños. Si bien tiene la ventaja de una mayor precisión, el número de iteraciones compromete la eficiencia general, ya que esto requiere más tiempo y cálculos para alcanzar el mínimo.\n",
    "\n",
    "La función de costo (o pérdida)  mide la diferencia, o error, entre la y real y la y pronosticada en su posición actual. Esto mejora la eficacia del modelo de machine learning , proporcionando feedback al modelo para que pueda ajustar los parámetros para minimizar el error y encontrar el mínimo local o global. Repite continuamente, moviéndose a lo largo de la dirección de descenso más pronunciado (o el gradiente negativo) hasta que la función de costo está cerca o en cero. En este punto, el modelo dejará de aprender. Además, si bien los términos función de costo y función de pérdida se consideran sinónimos, existe una ligera diferencia entre ellos. Vale la pena señalar que una función de pérdida se refiere al error de un ejemplo de entrenamiento, mientras que una función de costo calcula el error promedio en todo un conjunto de entrenamiento.\n",
    "\n",
    "#### 2.9.1  Tipos de descenso de gradiente\n",
    "Hay tres tipos de  algoritmos de aprendizaje de descenso de gradiente: descenso de gradiente por lotes, descenso de gradiente estocástico y descenso de gradiente por mini lotes.\n",
    "\n",
    "**Descenso de gradiente por lote** \n",
    " \n",
    "El descenso de gradiente por Lote  suma el error para cada punto en un conjunto de entrenamiento, actualizando el modelo solo después de que todos los ejemplos de entrenamiento  han sido evaluados. Este proceso se conoce como época de entrenamiento.\n",
    "\n",
    "Si bien este procesamiento por lotes proporciona eficiencia de cálculo, aún puede tener un tiempo de procesamiento prolongado para grandes conjuntos de datos de entrenamiento, ya que aún necesita almacenar todos los datos en la memoria. El descenso del gradiente por lotes también suele producir un gradiente de error estable y una convergencia, pero a veces ese punto de convergencia no es el ideal, encontrando el mínimo local versus el global.\n",
    "\n",
    "**Descenso de gradiente estocástico**\n",
    " \n",
    "El descenso de gradiente estocástico (SGD) ejecuta una época de entrenamiento para cada ejemplo dentro del conjunto de datos y actualiza cada  parámetro del ejemplo de entrenamiento, uno a la vez. Dado que solo necesita guardar un ejemplo de entrenamiento, es más fácil almacenarlos en la memoria. Si bien estas actualizaciones frecuentes pueden ofrecer más detalles y velocidad, pueden resultar en pérdidas en la eficiencia computacional en comparación con el descenso de gradiente de lote . Sus actualizaciones frecuentes pueden resultar en gradientes ruidosos, pero esto también puede ser útil para escapar del mínimo local y encontrar el global.\n",
    "\n",
    "**Descenso de gradiente de mini-lote**\n",
    "\n",
    "El descenso de gradiente de Mini-lote combina conceptos tanto del descenso de gradiente por lotes como del descenso de gradiente estocástico. Divide el conjunto de datos de entrenamiento en pequeños tamaños de lote  y realiza actualizaciones en cada uno de esos lotes. Este enfoque logra un equilibrio entre la eficiencia computacional del lote de descenso de gradiente y la velocidad del  descenso de gradiente estocástico.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Problema de interés: Detección de Melanoma mediante Redes Neuronales Convolucionales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Planteamiento del problema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Introducción\n",
    "\n",
    "En este proyecto nos enfocaremos en un problema importante dentro del contexto de la salud: la detección de melanoma, un tipo mortal de cáncer de piel. Esta forma de cáncer requiere una detección temprana y precisa para mejorar las tasas de superviviencia.\n",
    "\n",
    "La detección de melanoma se basa en la experiencia de dermatólogos y en el análisis visual de imágenes dermatoscópicas. Sabemos que este enfoque puede ser bastante subjetivo y que podría estar sujeto a errores.\n",
    "\n",
    "El objetivo del presente proyecto es desarrollar un modelo de aprendizaje automático basado en redes neuronales convolucionales (CNN) capaz de clasificar imágenes dermatoscópicas de 224 x 224 píxeles en dos categorías: melanoma y no melanoma. \n",
    "\n",
    "Las redes convolucionales son ampliamente utilizadas para tareas de clasificación de imágenes ya que aprenden características visuales que les permiten clasificar.\n",
    "\n",
    "Se utilizará un conjunto de datos previamente etiquetado para entrenar y evaluar el rendimiento del modelo.\n",
    "\n",
    "Durante la fase de entrenamiento, el modelo aprenderá a reconocer y extraer características relevantes de las imágenes. \n",
    "\n",
    "\n",
    "<center><img src=\"melanoma.jpeg\" width=300> <img src=\"no_melanoma.png\" width=300> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Metodología"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.1  Recopilación y procesamiento de datos\n",
    "\n",
    "Se utilizará un conjunto de datos previamente etiquetado que consta de imágenes dermatoscópicas de melanoma y no melanoma.\n",
    "\n",
    "El conjunto de datos se obtuvo de [Kaggle](https://www.kaggle.com/datasets/wanderdust/skin-lesion-analysis-toward-melanoma-detection), y consta de 5341 imágenes de melanoma y 5341 imágenes de no melanoma en el conjunto de entrenamiento, y 1781 imágenes para testing y validation en cada categoría. Las imágenes de melanoma son de 224 x 224 píxeles, mientras que las imagenes de no melanoma son de 600 x 450 pixeles por lo que realizaremos un preprocesamiento de re-size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.2 Diseño y entrenamiento de la red convolucional\n",
    "\n",
    "Se utilizará una red neuronal pre-entrenada llamada VGG16. Esta red se trata de una red convolucional con 16 capas.\n",
    "\n",
    "Se ajustarán los hiperparámetros, como la tasa de aprendizaje y el tamaño del lote, mediante técnicas de validación cruzada o búsqueda de hiperparámetros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.3 Evaluación y validación del modelo\n",
    "\n",
    "El modelo entrenado se evaluará utilizando la métrica de desempeño F1. Además, se analizarán las matrices de confusión para evaluar la capacidad del modelo para distinguir entre melanoma y no melanoma. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.4 Pruebas y análisis adicionales\n",
    "\n",
    "Una vez que el modelo haya sido entrenado y evaluado, se realizarán pruebas adicionales utilizando imágenes de prueba no vistas previamente para validar la capacidad del modelo para generalizar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Desarrollo matemático de la red neuronal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las redes convolucionales (LeCun, 1989), también conocidas como redes neuronales convolucionales o CNN, son un tipo especializado de red neuronal diseñada para procesar datos que tienen una topología conocida en forma de una malla o cuadrícula.\n",
    "\n",
    "<center><img src=\"VGG16_1.png\" width=600> </center>\n",
    "\n",
    "Comencemos explicando cada elemento básico dentro de la red neuronal convolucional.\n",
    "\n",
    "<center><img src=\"Basic_CNN.png\" width=600> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1  **Capa de entrada**\n",
    "\n",
    "Las entradas de la red neuronal VGG16 corresponden con las imagenes de $224 \\times 224$ pixeles. \n",
    "\n",
    "Recordemos que los colores se miden en escala RGB, es decir, cada pixel dentro de una imágen va a estar asociado a tres valores (rojo, verde y azul), así, podemos pensar a una imágen de 224x224 como un objeto que para cada canal de color posee una matriz de valores.\n",
    "\n",
    "Dentro de procesamiento de imágenes, una imagen a color puede representarse como un tensor de tres dimensiones (generalización de matrices en tres deimensiones).\n",
    "\n",
    "<center><img src=\"imagenrgb.png\" width=300> </center>\n",
    "\n",
    "Podemos definir tal tensor de la siguiente manera.\n",
    "Sea $\\mathbb{Z}_{255} = \\{x \\in \\mathbb{Z} |  0\\le x\\le 255\\}$, es decir, el conjunto de los enteros que van desde 0 hasta 255. Definimos al tensor $X_{i,j,k}$ como $$X_{i,j,k} \\in \\mathbb{Z}_{255}^{224 \\times 224 \\times 3},$$\n",
    "Donde:\n",
    "- $X_{i,j,k}$ representa la imagen de entrada a la red convolucional. \n",
    "- $k$ es el subíndice que representa el canal de color, por lo que al fijar el valor de $k$, digamos $X_{i,j, 1}$, representará una matriz de $i\\times j$ en el canal rojo en la escala RGB, que tiene como entradas valores enteros que van desde 0 hasta 255.\n",
    "- $i$ es el renglón de la matriz, es decir, es la posición vertical de cada pixel. \n",
    "- $j$ representa la columna de la matriz, es decir, la posición horizontal del pixel.\n",
    "\n",
    "<center><img src=\"tensores.png\" width=300> </center>\n",
    "\n",
    "En lo siguiente, haremos referencia a la representación en forma de función $I(x,y)$ de una imagen. Definimos $$I(x,y) = (R(x,y), G(x,y), B(x,y)),$$ donde $(x,y)$ son las coordenadas espaciales de un pixel dentro de la imagen y $(R, G, B)$ son los valores de color correspondientes en ese pixel. \n",
    "\n",
    "Para realizar la conexión de ambas representaciones podemos pensar en una imagen como una función que a cada pixel $(x,y)$ le asigna una tripleta de valores numéricos $(R,G,B)$. De esta manera, podemos pensar a $I$ como una función $I: \\mathbb{Z}^2 \\to \\mathbb{Z}_{255}^3$, donde\n",
    "$$R(x, y) = X_{x, y, 1}$$\n",
    "$$G(x, y) = X_{x, y, 2}$$\n",
    "$$B(x, y) = X_{x, y, 3}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 **Capa convolucional**\n",
    "\n",
    "Esta capa es la primera capa oculta que se utiliza para extraer características de la imagen de entrada.Se compone de \n",
    "#### **Filtros:** \n",
    "\n",
    "También conocidos como núcleos o kernels, son matrices de pesos que se deslizan sobre los datos de entrada. Cada filtro extrae características específicas de los datos mediante la operación de convolución.\n",
    "\n",
    "<center><img src=\"Kernels.png\" width=600> </center> \n",
    "\n",
    "En el contexto de procesamiento de imágenes, un filtro es una matriz o tensor que contiene valores numéricos utilizados para realizar la convolución con una imagen. Estos valores numéricos especifican cómo se deben combinar los píxeles vecinos para obtener los nuevos valores de píxeles en la imagen de salida. Los filtros se utilizan para realizar diversas operaciones de procesamiento de imágenes, como el filtrado espacial, la detección de bordes, la suavización, entre otros.\n",
    "\n",
    "Estos representan los pesos que la red neuronal va a aprender.\n",
    "\n",
    "<center><img src=\"Filtro.png\" width=600> </center>\n",
    "\n",
    "\n",
    "#### **Operación de convolución:** \n",
    "Es la operación fundamental realizada en una capa convolucional. Consiste en deslizar los filtros sobre los datos de entrada y calcular la suma ponderada de los elementos de la entrada cubiertos por el filtro en cada posición.\n",
    "\n",
    "En las aplicaciones de procesamiento de imagen, la convolución de distintos “filtros\" sobre una imagen es una herramienta habitual empleada en problemas de segmentación, detección de bordes o reducción de imagen. No obstante, los valores de\n",
    "dichos filtros deben ser establecidos a priori, lo que requiere experiencia en el campo\n",
    "y a menudo largos procesos de ajuste de parámetros al problema en cuestión. En\n",
    "contraposición, las redes convolucionales parten de una serie de filtros aleatorios\n",
    "que se ajustan de manera automática mediante el proceso de aprendizaje de la red.\n",
    "\n",
    "En matemáticas, una convolución es una operación matemática sobre dos funciones (f y g) que produce una tercera función que expresa como la forma de una de ellas fue modificada por la otra. En otras palabras, una convolución es una manera de aplicar un filtro a una función y obtener otra función como resultado.\n",
    "\n",
    "En particular, en procesamiento de imágenes, una convolución es una operación matemática que se utiliza para combinar una imagen de entrada con un filtro o kernel para obtener una imagen de salida. La convolución se realiza deslizando el kernel sobre la imagen de entrada y calculando el producto punto entre los elementos del kernel y los píxeles correspondientes en la imagen. Luego, se suma el resultado de estos productos para obtener el valor de cada píxel en la imagen de salida.\n",
    "\n",
    "Para definir estos elementos, nos concentraremos en un solo canal de color de la imagen para trabajar en elementos de dos dimensiones. Fijamos el canal $c$, de esta manera $X = (x_{i,j}) \\in M_{i\\times j}(\\mathbb{Z}_{255})$ representa una matriz de $i\\times j$ con entradas enteras no negativas de 0 a 255 en el canal de color fijo $c$. \n",
    "\n",
    "Así, de manera correspondiente, la representación funcional de la imagen quedaría definida como, $I: \\mathbb{Z}^2 \\to \\mathbb{Z}_{255}$, con $$I(i,j) := x_{i,j}$$\n",
    "\n",
    "Un kernel dentro de este contexto es una matriz cuadrada pequeña que se aplica a la imagen mediante la convolución. Podríamos pensar en este kernel como un filtro, que tiene un efecto sobre la imagen de entrada. Cada elemento del kernel especifica el peso o la contribución relativa de un píxel vecino en la generación del valor de un píxel en la imagen de salida.\n",
    "\n",
    "**Definición de convolución:** \n",
    "Formalmente, la convolución de una imagen de entrada $I(i, j)$ con una matriz de $n\\times m$ llamado kernel (filtro) $K(m, n)$, se denota por $S=I*K$ y se puede definir como sigue:\n",
    "$$S(i, j) = (I*K)(i,j)= \\sum_{m} \\sum_{n} I(i-m, j-n) K(m, n)\n",
    "$$\n",
    "\n",
    "<center><img src=\"2D_Convolution_Animation.gif\" width=600> </center>\n",
    "\n",
    "En la terminología de las redes convolucionales, el primer argumento (en este ejemplo, la función I) de la convolución se suele denominar entrada, y el segundo argumento (en este ejemplo, la función K) se llama núcleo o filtro. La salida a veces se denomina mapa de características.\n",
    "\n",
    "La convolución con imágenes a color se define de manera similar a la convolución con imágenes en escala de grises, pero se realiza teniendo en cuenta los canales de color adicionales. En este caso, se realizan operaciones de convolución separadas en cada canal de color de la imagen.\n",
    "\n",
    "Supongamos que tenemos una imagen a color compuesta por tres canales: rojo (R), verde (G) y azul (B). Para realizar la convolución con una imagen a color, se aplicará la convolución de manera independiente en cada canal de color utilizando un filtro o kernel correspondiente.\n",
    "\n",
    "Cada capa de convolución dentro de la red neuronal VGG16 está compuesto con filtros de $3\\times 3$, así una capa de convolución está compuesta por $N$ filtros $W$ de dimensión $3 \\times 3 \\times 3$.\n",
    "Dada una entrada $X$ de dimensión $h \\times w \\times 3$, cada uno de los filtros de convolución\n",
    "se coloca sobre cada ventana de tamaño $3 \\times 3 \\times 3$ centrada en cada píxel $x_{i,j}$ de $X$.\n",
    "El filtro de convolución se va desplazando a lo largo de toda la imagen, tras aplicar\n",
    "la convolución a cada una de las ventanas de la imagen. La salida $Z_{i,j}$ generada\n",
    "para la ventana centrada en el pixel $x_{i,j}$ se calcula en base a la siguiente fórmula:\n",
    "$$Z_{i,j} = \\sum_{d=1}^3 \\sum_{n=1}^3 \\sum_{m=1}^3 X_{i+m-\\frac{k+1}{2}, j+n-\\frac{k+1}{2}, d}\\cdot W_{m,n,d}$$\n",
    "\n",
    "Al resultado $Z$ producido por un filtro de convolución se le llama imagen de\n",
    "características, dado que ofrece información sobre la presencia o ausencia de la característica buscada por el filtro en cuestión. Dado que se cuenta con $N$ filtros de convolución, se generarán $N$ imágenes de características que, para poder ser tratadas de\n",
    "manera cómoda por las siguientes capas de la red, se apilan unas con otras, dando\n",
    "lugar a una imagen de características tridimensional. Así, el canal $d$-ésimo de $Z$\n",
    "contiene el resultado de aplicar el filtro de convolución $d$-ésimo a $X$\n",
    "\n",
    "#### **Mapas de características:** \n",
    "Cada filtro produce un mapa de características, que es una representación en 2D de las características extraídas en cada posición. Cada mapa de características se obtiene aplicando el filtro correspondiente a la entrada y realizando la convolución. Podríamos pensar al mapa de características como la salida que obtenemos a partir de la convolución.\n",
    "\n",
    "#### **Función de activación ReLU:** \n",
    "Después de la convolución, se aplica una función de activación no lineal a cada elemento del mapa de características para introducir no linealidad en la red. La función de activación más comúnmente utilizada en las capas convolucionales es la función ReLU (Rectified Linear Unit), y es la que utiliza la red propuesta.\n",
    "\n",
    "Recordemos que la función ReLU se define como $$f(x)=\\max(0, x)$$\n",
    "\n",
    "<center><img src=\"ReLU.png\" width=600> </center>\n",
    "\n",
    "Podríamos pensar en la función de activación como una forma de asignar puntuaciones a los valores de píxeles según alguna medida de importancia. La activación ReLU establece que los valores negativos no son importantes y los convierte en 0. Esta función se aplica a cada término del mapa de características.\n",
    "\n",
    "Existen varias razones para utilizar esta función de activación y todas estas tienen que ver con facilitar el trabajo posterior:\n",
    "\n",
    "- **No linealidad:** ReLU introduce no linealidad a la red, lo que permite que la red aprenda representaciones más complejas y expresivas. ReLU es una función no lineal simple pero efectiva que introduce no linealidad sin ser computacionalmente costosa.\n",
    "\n",
    "- **Activación dispersa:** ReLU proporciona dispersión en las activaciones. Dado que ReLU establece los valores negativos en cero, introduce dispersión en la red, lo que significa que solo un subconjunto de neuronas se activa. Las activaciones dispersas ayudan a reducir la complejidad computacional, los requisitos de memoria y el sobreajuste al enfocarse en las características más relevantes.\n",
    "\n",
    "- **Problema del gradiente desvaneciente:** ReLU ayuda a mitigar el problema del gradiente desvaneciente, que puede ocurrir durante la retropropagación en redes neuronales profundas. La derivada de ReLU es 0 o 1, proporcionando un gradiente constante para entradas positivas. Esto evita la saturación de los gradientes y facilita un mejor flujo de gradientes, lo que permite un entrenamiento más estable y eficiente.\n",
    "\n",
    "- **Eficiencia:** ReLU es computacionalmente eficiente en comparación con otras funciones de activación como sigmoidal o tangente hiperbólica, ya que involucra operaciones simples elemento a elemento y no requiere cálculos exponenciales costosos.\n",
    "\n",
    "\n",
    "En la red VGG16 se tienen 5 capas convolucionales, con una cantidad pequeña de filtros de $3\\times 3$.\n",
    "<center><img src=\"conv1.png\" width=800> </center>\n",
    "<center><img src=\"esquema_conv.png\" width=600> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Capa de agrupamiento (Max Pooling)\n",
    "\n",
    "Para que el proceso anterior sea útil, el número de filtros aprendidos en la red deberá ser elevado, y por consiguiente, la dimensionalidad de la imagen de características aumentará notablemente. Dado que la eficiencia y facilidad de entrenamiento del clasificador de nuestra red mejorarán si se opera sobre un vector de características de pocos atributos, resulta necesario reducir la imagen de características generada. Es aquí donde las capas de “pooling\" entran en juego.\n",
    "\n",
    "La capa de agrupamiento Max Pooling se realiza dividiendo cada mapa de características en regiones no superpuestas y tomando el valor máximo de cada región, esto es, para cada región se selecciona únicamente el valor máximo y se descartan los otros valores. El resultado de esta capa es un mapa de características con dimensionalidad reducida. La función de estas capas es la de reducir la imagen de características gradualmente, tratando de preservar la máxima información significativa posible.\n",
    "\n",
    "Esta capa de agrupamiento es una operación comunmente utilizada entre las capas intermedias convolucionales, y su principal función es reducir la dimensionalidad espacial de los mapas de características y poder extraer las características más relevantes en una red convolucional.\n",
    "\n",
    "El max pooling es una operación no lineal y no tiene parámetros entrenables.\n",
    "\n",
    "En la red VGG16, el maxpooling que se utiliza es de tamaño de agrupamiento de $2\\times 2$ y con stride de $2$. Esto significa que este Max Pooling divide la entrada en ventanas disjuntas considerando un paso o stride de 2 espacios. Por esta razón nos limitaremos a este tipo de pooling. Un dato adicional es que este pooling reduce la dimensionalidad en un factor de $2\\times 2$.\n",
    "\n",
    "Dada una entrada $A$ de tamaño $H\\times W \\times C$ donde $H\\times W$ es la dimensión espacial y $C$ el número de canales, y un tamaño de agrupamiento o ventana $2\\times 2$, con tamaño de paso $2$, la operación de agrupamiento puede representarse de la siguiente manera.\n",
    "\n",
    "La matriz de entrada $A$ se descompone en ventanas disjuntas que denotaremos por $A^{\\alpha \\beta}$ de dimensión $2\\times 2 \\times c$. Por cada canal se lleva a cabo la siguiente operación:\n",
    "$$Y^{\\alpha \\beta}_c = \\max(X^{\\alpha \\beta}_c)$$ \n",
    "\n",
    "El paso de $A$ a través de una capa de pooling de tamaño $k \\times k$, reduce el número\n",
    "de atributos por un factor de $k \\times k$. Las sucesivas capas de pooling de la red siguen\n",
    "reduciendo el número de atributos, hasta llegar a un número aceptable para el clasificador de la red.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Capa Flatten\n",
    "\n",
    "Después de terminar las capas precias, deberíamos tener un mapa de características agrupadas (pooled). En la capa flatten, como su nombre lo indica, vamos a \"aplanar\" nuestro mapa de características a una sola columna, como se muestra en la imagen:\n",
    "\n",
    "<center><img src=\"Flattening.png\" width=600> </center>\n",
    "\n",
    "\n",
    "La idea de la capa flatten es simplemente pasar de matrices o tensores a un solo vector, es decir, se trata solamente de un reacomodo de los elementos del tensor para obtener un vector de unidimensional. Es importante destacar que en esta capa no se introducen parámetros entrenables, ya que solo se trata de un reacomodo. \n",
    "\n",
    "La capa Flatten se utiliza para permitir un procesamiento adicional en capas completamente conectadas dentro de la red convolucional.\n",
    "\n",
    "<center><img src=\"flattening1.png\" width=600> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Capa densa (Fully-connected)\n",
    "\n",
    "Las capas fully-connected dentro de las redes convolucionales se encuentran tipicamente al final de la arquitectura después de la capa flattening. Son aquellas capas en las que todas las entradas de una capa están conectadas a cada unidad de activación de la siguiente capa. En la mayoría de los modelos populares de aprendizaje automático, las últimas capas son capas fully connected, que compilan los datos extraídos por las capas anteriores para formar la salida final.\n",
    "\n",
    "Después de nuestras 13 capas convolucionales, conectamos nuestra capa plana a 3 capas fully connected (totalmente conectadas). Una pregunta muy natural es por qué usamos capas fully connected en absoluto. La razón por la que no querríamos usarlas es la enorme cantidad de parámetros de peso en la primera capa fully connected:\n",
    "\n",
    "$512 * 7 * 7 * 4096 = 102,760,448$ parámetros de peso conectan la capa plana a la primera capa fully connected.\n",
    "\n",
    "Podemos pensar en las capas convolucionales y de pooling como la creación de representaciones útiles de los datos. Recuerda que ambas operaciones son locales en el sentido de que tienen en cuenta ventanas de los datos. Las capas fully connected, en cambio, son globales y conectan cada valor en la capa de max pooling anterior ($\\bold{m}^{[13]}$) juntos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Capa de salida\n",
    "\n",
    "El último paso es conectar nuestra última capa fully connected ($\\bold{a}^{[16]}$) a nuestra capa de salida ($\\hat{y}$). Para hacer esta transición, debemos usar una función de activación. \n",
    "\n",
    "En la arquitectura VGG16 se utiliza la función de activación softmax en la última cpa, pero dado que nuestro problema se trata de clasificación binaria, cambiamos esta capa por una sola neurona con función de activación sigmoidal, lo cual que nos dará como salida un solo valor $\\hat{y}$ que representa la probabilidad de pertenencia a la clase \"melanoma\".\n",
    "\n",
    "Veamos un diagrama que describe la arquitectura de la red convolucional a utilizar:\n",
    "\n",
    "<center><img src=\"VGG_15.png\" width=400> </center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizaremos el desarrollo matemático de la red neuronal que utilizamos.\n",
    "\n",
    "\n",
    "Primero, vamos a definir los símbolos utilizados en las ecuaciones:\n",
    "\n",
    "- $X$ representa la entrada a la red neuronal convolucional.\n",
    "- $W$ representa los pesos de la capa convolucional.\n",
    "- $b$ representa los sesgos (biases) de la capa convolucional.\n",
    "- $A$ representa la salida de la capa convolucional.\n",
    "- $\\sigma$ representa la función de activación, que en este caso asumiremos que es la función ReLU.\n",
    "- $\\ast$ representa la operación de convolución.\n",
    "- $\\oplus$ representa la operación de suma.\n",
    "\n",
    "A continuación se muestra el desarrollo matemático de una CNN16:\n",
    "\n",
    "1. Convolución de la primera capa:\n",
    "$$Z^{[1]} = X \\ast W^{[1]} + b^{[1]}$$\n",
    "$$A^{[1]} = \\sigma(Z^{[1]})$$\n",
    "\n",
    "2. Convolución de la segunda capa:\n",
    "$$Z^{[2]} = A^{[1]} \\ast W^{[2]} + b^{[2]}$$\n",
    "$$A^{[2]} = \\sigma(Z^{[2]})$$\n",
    "\n",
    "3. Capa de agrupación (pooling) de la segunda capa:\n",
    "$$A^{[2]}_{\\text{pooled}} = \\text{MaxPooling}(A^{[2]})$$\n",
    "\n",
    "4. Convolución de la tercera capa:\n",
    "$$Z^{[3]} = A^{[2]}_{\\text{pooled}} \\ast W^{[3]} + b^{[3]}$$\n",
    "$$A^{[3]} = \\sigma(Z^{[3]})$$\n",
    "\n",
    "5. Convolución de la cuarta capa:\n",
    "$$Z^{[4]} = A^{[3]} \\ast W^{[4]} + b^{[4]}$$\n",
    "$$A^{[4]} = \\sigma(Z^{[4]})$$\n",
    "\n",
    "6. Capa de agrupación de la cuarta capa:\n",
    "$$A^{[4]}_{\\text{pooled}} = \\text{MaxPooling}(A^{[4]})$$\n",
    "\n",
    "7. Repite los pasos 4-6 para las capas 5-8.\n",
    "\n",
    "8. Convolución de la novena capa:\n",
    "$$Z^{[9]} = A^{[8]} \\ast W^{[9]} + b^{[9]}$$\n",
    "$$A^{[9]} = \\sigma(Z^{[9]})$$\n",
    "\n",
    "9. Convolución de la décima capa:\n",
    "$$Z^{[10]} = A^{[9]} \\ast W^{[10]} + b^{[10]}$$\n",
    "$$A^{[10]} = \\sigma(Z^{[10]})$$\n",
    "\n",
    "10. Capa de agrupación de la décima capa:\n",
    "$$A^{[10]}_{\\text{pooled}} = \\text{MaxPooling}(A^{[10]})$$\n",
    "\n",
    "11. Repite los pasos 9-10 para las capas 11-14.\n",
    "\n",
    "12. Convolución de la decimoquinta capa:\n",
    "$$Z^{[15]} = A^{[14]} \\ast W^{[15]} + b^{[15]}$$\n",
    "$$A^{[15]} = \\sigma(Z^{[15]})$$\n",
    "\n",
    "13. Convolución de la decimosexta capa:\n",
    "$$Z^{[16]} = A^{[15]} * W^{[16]} + b^{[16]}$$\n",
    "$$A^{[16]} = σ(Z^{[16]})$$\n",
    "\n",
    "Estas ecuaciones representan el desarrollo matemático de una red neuronal convolucional (CNN) con 16 capas. Cada capa realiza una convolución seguida de una función de activación ReLU, y en algunas capas se realiza también una capa de agrupación (pooling) para reducir la dimensionalidad de las características extraídas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Bibliografía"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://oa.upm.es/71361/1/TFG_LUIS_MERINO_ULIZARNA.pdf\n",
    "\n",
    "- https://www.youtube.com/watch?v=eNIqz_noix8\n",
    "\n",
    "- https://www.youtube.com/watch?v=M5QHwkkHgAA&t=3s\n",
    "\n",
    "- https://towardsdatascience.com/cross-entropy-loss-function-f38c4ec8643e\n",
    "\n",
    "- https://rstudio-pubs-static.s3.amazonaws.com/599210_4306c5d3d6a34a6c829566ca11b7e27a.html#1\n",
    "\n",
    "- https://medium.com/@draj0718/convolutional-neural-networks-cnn-architectures-explained-716fb197b243\n",
    "\n",
    "- https://www.ibm.com/mx-es/topics/gradient-descent\n",
    "\n",
    "- https://www.ibm.com/mx-es/topics/neural-networks\n",
    "\n",
    "- https://www.aprendemachinelearning.com/como-funcionan-las-convolutional-neural-networks-vision-por-ordenador/\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
